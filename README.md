# ğŸ”„ Self-Refinement Loop Project

A system that iteratively improves AI-generated answers using a self-refinement loop and a separate ML critic.

## ğŸ“ Project Structure

```
refine-loop/
â”œâ”€â”€ config/                    # Configuration files
â”‚   â”œâ”€â”€ API_KEY.txt           # Gemini API key
â”‚   â””â”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ data/                      # Input data
â”‚   â”œâ”€â”€ prompts.csv           # Training prompts
â”‚   â””â”€â”€ test_prompts.csv      # Test prompts
â”œâ”€â”€ models/                    # Trained models
â”‚   â”œâ”€â”€ improved_svm_critic_model.pkl
â”‚   â”œâ”€â”€ improved_svm_critic_vectorizer.pkl
â”‚   â””â”€â”€ improved_svm_text_extractor.pkl
â”œâ”€â”€ logs/                      # Output logs and results
â”œâ”€â”€ scripts/                   # Main execution scripts
â”‚   â”œâ”€â”€ run_analysis.py       # Analysis launcher
â”‚   â”œâ”€â”€ run_demo.py           # Demo launcher
â”‚   â””â”€â”€ test_improved_critic.py # Critic testing
â”œâ”€â”€ src/                       # Source code
â”‚   â”œâ”€â”€ core/                  # Core functionality
â”‚   â”‚   â”œâ”€â”€ critics/          # Critic implementations
â”‚   â”‚   â”œâ”€â”€ refinement/       # Refinement logic
â”‚   â”‚   â””â”€â”€ training/         # Model training
â”‚   â”œâ”€â”€ analysis/             # Analysis tools
â”‚   â”œâ”€â”€ utils/                # Utilities
â”‚   â””â”€â”€ ui/                   # User interface
â””â”€â”€ docs/                     # Documentation
    â”œâ”€â”€ DETAILED_README.md    # Detailed documentation
    â””â”€â”€ IMPROVEMENTS_SUMMARY.md
```

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Clone the repository
git clone <your-repo-url>
cd refine-loop

# Install dependencies
pip install -r config/requirements.txt

# Create a .env with your key (preferred)
echo "GOOGLE_API_KEY=your-api-key-here" > .env
```

### 2. Run Analysis

```bash
# Run analysis tools
python scripts/run_analysis.py

# Or run individual analyses
python src/analysis/simple_analysis.py
```

### 3. Launch Interactive Demo

```bash
# Start the Streamlit demo
python scripts/run_demo.py

# Or run directly with streamlit
streamlit run ui/demo_app.py
```

### 4. Reproduce Results

```bash
# End-to-end reproduction (generate data â†’ train critic â†’ eval â†’ print metrics)
bash scripts/reproduce_results.sh
```

## ğŸ“Š Current Performance

- **Success Rate**: 60.7% (answers deemed sufficient by critic)
- **Average BLEU**: 0.64
- **Average ROUGE-L**: 0.813
- **Critic**: Improved SVM with TFâ€‘IDF + handcrafted features; threshold 0.67

## ğŸ”§ Key Components

### Core Functionality (`src/core/`)
- **Critics** (`critics/`): ML-powered critic implementations
- **Refinement** (`refinement/`): Self-refinement logic and batch processing
- **Training** (`training/`): Model training utilities

### Analysis Tools (`src/analysis/`)
- Performance evaluation and visualization
- Model improvement analysis
- Results analysis and metrics

### User Interface (`ui/`)
- Interactive Streamlit demo

## ğŸ“š Documentation

For more depth, see:
- `docs/DETAILED_README.md` â€“ Comprehensive project documentation
- `docs/IMPROVEMENTS_SUMMARY.md` â€“ Recent improvements and next steps

## ğŸ¯ Features

- Batch processing with checkpointing
- ML-powered critics for answer quality
- Analysis tools and visualizations
- Interactive demo for quick testing

## ğŸ“ˆ Performance Metrics

- Success Rate: 60.7%
- BLEU: 0.64
- ROUGEâ€‘L: 0.813
- Improved Critic Training Accuracy: 98.8%

The project is organized for maintainability and easy navigation.
